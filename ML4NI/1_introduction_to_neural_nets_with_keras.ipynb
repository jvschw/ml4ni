{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EmCaF0WmwF4"
      },
      "source": [
        "**Notebook 1 – Introduction to Artificial Neural Networks with Keras**\n",
        "\n",
        "_This notebook introduces ANNs._\n",
        "jens.schwarzbach@ukr.de"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sxY6DiFmwF5"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jvschw/ml4ni/blob/master/ML4NI/1_introduction_to_neural_nets_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u86EFSK6mwF6"
      },
      "source": [
        "# Intro\n",
        "To correctly classify images (such as \"this is a house\") is a difficult task at which biological organisms used to outperform machines. This has changed since developers have begun to mimic biological systems in machine learning by means of artificial neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lizPGLkzmwF6"
      },
      "source": [
        "Advance with SHIFT ENTER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN3UBjXdmwF7"
      },
      "source": [
        "## Biological and artificial neurons "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT4w8RuUmwF7"
      },
      "source": [
        "An Artificial Neural Network (ANN) is a Machine Learning Model inspired by the networks of biological neurons found in the brain. Neurons receive input through the synapses on their dentritic branches, which leads to changes of the neuron's membrane potential. If the membrane potential falls below a certain threshold, the neuron creates an action potential, which travels down the axon and eventually reaches the synaptic terminals where the neuron makes contact with other neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj8REp1FmwF7"
      },
      "source": [
        "<img src=\"https://github.com/jvschw/ml4ni/blob/master/ML4NI/images/ann/Blausen_0657_MultipolarNeuron_small2.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUoIK_nmmwF8"
      },
      "source": [
        "Neuronal activity is frequency encoded. This means, the more excitatory input the neuron receives the higher its firing rate (measured in spikes per second or Hz) becomes. Below, you see how the spike rate (scaled to 100%) of four neurons in primary visual cortex (V1) increases as a function of stimulus contrast [(Albrecht & Hamilton, 1982)](https://www.physiology.org/doi/abs/10.1152/jn.1982.48.1.217)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQc_mcJSmwF8"
      },
      "source": [
        "<img src=\"https://github.com/jvschw/ml4ni/blob/master/ML4NI/images/ann/CRF_Albrecht_Hamilton_1982_small.jpg?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2XO5oRFmwF8"
      },
      "source": [
        "This input-output dependency can be captured mathematically by a perceptron, an artificial neuron that computes the weighted sum of its inputs (input x synaptic strength) which then is passed to an activation function, which determines the output of that neuron. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efjjgnENmwF9"
      },
      "source": [
        "<img src=\"https://github.com/jvschw/ml4ni/blob/master/ML4NI/images/ann/Rosenblattperceptron.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV6jfcvBmwF9"
      },
      "source": [
        "Artificial neurons can be equipped with a host of different activation functions. The left panel in the figure below shows some of the most common activation functions, the right panel shows their respective derivatives.\n",
        "Derivatives are important, since learning yields the strongest changes where the derivative is highest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjt2wWkmwF9"
      },
      "source": [
        "<img src=\"https://github.com/jvschw/ml4ni/blob/master/ML4NI/images/ann/activation_functions_plot.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPhoe56imwF-"
      },
      "source": [
        "## Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIJ_WunfmwF-"
      },
      "source": [
        "ANNs consist of different layers of (artificial) neurons, typically one input layer (red), one output layer (blue), and a variable number of hidden layers (yellow) in between. Neurons in one layer are connected to neurons in subsequent layers by means of (synaptic) weights. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar1iWG47mwF-"
      },
      "source": [
        "<img src=\"https://github.com/jvschw/ml4ni/blob/master/ML4NI/images/ann/ANN_example.jpg?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2rmRJ3MmwF_"
      },
      "source": [
        "# Simulating ANNs with KERAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT50k6GvmwF_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3LrcINtmwF_"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dg-rkbRmwGA"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"ann\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LTNYle7mwGB"
      },
      "source": [
        "**Note**: we set `max_iter` and `tol` explicitly to avoid warnings about the fact that their default value will change in future versions of Scikit-Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI14zwWlmwGC"
      },
      "source": [
        "## Building an Image Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqUFzj2vmwGC"
      },
      "source": [
        "First let's import TensorFlow and Keras.\n",
        "\n",
        "Tensorflow is a library for numerical computation developed by the Google Brain Team. In mathematics, a tensor is an algebraic object that describes a linear mapping from one set of algebraic objects (e.g.  a scalar, vector or matrix) to another. In case of ANNs this means how the activity of one neuronal layer is transformed to another layer.\n",
        "\n",
        "Keras is a high-level neural networks application programming interface (API), written in Python and capable of running on top of TensorFlow, and other libraries such as CNTK or Theano. It was developed with a focus on enabling fast experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JWxlCl6mwGC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itRJLO74mwGD"
      },
      "outputs": [],
      "source": [
        "#which version is installed?\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzShw0I8mwGD"
      },
      "outputs": [],
      "source": [
        "#which version is installed?\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvBc91HCmwGD"
      },
      "source": [
        "Now, let's load the fashion MNIST dataset. Keras has a number of functions to load popular datasets in `keras.datasets`. \n",
        "The training data is a matrix (X) with the dimensions [nSamples, nFeatures]. The labels are contained in a vector (y) with nFeatures elements.\n",
        "The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVqkt_KQmwGE"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-CeEVKTmwGE"
      },
      "source": [
        "The training set contains 60,000 grayscale images, each 28x28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQQc_7IhmwGE"
      },
      "outputs": [],
      "source": [
        "X_train_full.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq9ZtQA9mwGE"
      },
      "source": [
        "and of 60,000 class labels, each of which being a number from 0-9: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cyjrI-vmwGF"
      },
      "source": [
        "The test set contains 10,000 grayscale images, each 28x28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHfYQnxDmwGF"
      },
      "outputs": [],
      "source": [
        "y_train_full.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhDFYmp5mwGF"
      },
      "source": [
        "The test set contains 10,000 grayscale images, each 28x28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CFPB7YwmwGF"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKyGleSJmwGG"
      },
      "source": [
        "and 10,000 class labels, each of which being a number from 0-9: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOTj4qedmwGG"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HUkXzstmwGH"
      },
      "source": [
        "Each pixel intensity is represented as a byte (0 to 255):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkGxPZ1nmwGI"
      },
      "outputs": [],
      "source": [
        "X_train_full.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrrhua8imwGJ"
      },
      "source": [
        "Let's split the full training set into a validation set and a (smaller) training set. We also scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaY9PJGfmwGJ"
      },
      "outputs": [],
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHxNXVHamwGK"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOsaZOGVmwGK"
      },
      "outputs": [],
      "source": [
        "X_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBjQXxCBmwGK"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDh9ld7mwGM"
      },
      "source": [
        "You can plot an image using Matplotlib's `imshow()` function, with a `'binary'`\n",
        " color map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6lrIoX5mwGN"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_train[0], cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_WXsx4JmwGO"
      },
      "source": [
        "The labels are the class IDs (represented as uint8), from 0 to 9:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jwe-jHJmwGO"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g1qV1RnmwGP"
      },
      "source": [
        "Here are the corresponding class names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDzOjQLnmwGP"
      },
      "outputs": [],
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_HEbqmomwGP"
      },
      "source": [
        "So the first image in the training set is a coat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN4H1aeDmwGP"
      },
      "outputs": [],
      "source": [
        "class_names[y_train[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyzg14rvmwGQ"
      },
      "source": [
        "The validation set contains 5,000 images, and the test set contains 10,000 images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkNelXjAmwGQ"
      },
      "outputs": [],
      "source": [
        "X_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdt72d3QmwGQ"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPGJ4gtVmwGQ"
      },
      "source": [
        "Let's take a look at a sample of the images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_FG3uY6mwGQ"
      },
      "outputs": [],
      "source": [
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "save_fig('fashion_mnist_plot', tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsaMrpXCmwGQ"
      },
      "source": [
        "We start a keras session and initialize the random-seed generator to a fixed value such that we can replicate the session.\n",
        "Then we build a sequential model with keras. Sequential means that the information flows sequentially from one layer to the next.\n",
        "We start with an input layer, then we add two hidden layers (the first with 300 neurons, the second with 100 neurons), which are densely connected (dense means each unit of a given layer is connected to each unit to the preceding layer, as opposed to sparse connections). Both hidden layers use the relu activation function. Finally, we add an output layer with 10 neurons (because we have 10 output categories) and a softmax activation function, which gives us the probability of the class a neuron represents (all neurons sum up to a probability of 1).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG1mLN4xmwGR"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arEaFWQRmwGR"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytJ41toUmwGR"
      },
      "source": [
        "'model.summary' provides us with a text-description of our model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugB1wfWsmwGR"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLRHrGjrmwGR"
      },
      "source": [
        "Keras has a function that produces a graphical depiction of a model. [Note: the ? in the numerical descriptions denotes the batch-size, which we have not defined yet. More about batch-sizes further below when we talk about the model's optimizing function.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNalO8VumwGd"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"my_mnist_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUo_hjugmwGd"
      },
      "source": [
        "We can access layers and use variables to refer to them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYPmQc7MmwGd"
      },
      "outputs": [],
      "source": [
        "hidden1 = model.layers[1]\n",
        "hidden1.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6FiisxImwGd"
      },
      "outputs": [],
      "source": [
        "model.get_layer(hidden1.name) is hidden1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbMi6KSbmwGd"
      },
      "outputs": [],
      "source": [
        "weights, biases = hidden1.get_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmzZJLm7mwGe"
      },
      "source": [
        "Weights are initialized with small random values, otherwise there would not be any gradients and the model would be unable to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QE8DjWemwGe"
      },
      "outputs": [],
      "source": [
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR4MmPv-mwGe"
      },
      "source": [
        "The first hidden layer gets its input from the input layer, which has 784 (i.e. 28x28) units. The first hidden layer itself has 300 units. We can look at the shape of the weight matrix. You see it is organized as (from, to)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsoBNVlZmwGe"
      },
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FIZNUL5mwGe"
      },
      "source": [
        "Biases are initialized with zeroes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdHvT4clmwGe"
      },
      "outputs": [],
      "source": [
        "biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHnM6GF_mwGe"
      },
      "source": [
        "Each unit has its own bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw1PaKBImwGe"
      },
      "outputs": [],
      "source": [
        "biases.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg0Z5WkumwGe"
      },
      "source": [
        "Now we are ready to compile the model, for which we have to provide three more pieces of information: 1) the loss function (here sparse_categorical_crossentropy), 2) the optimizer for learning (here stochastic gradient descent), and 3) the performance metrics (here accuracy).\n",
        "\n",
        "Why 'sparse_categorical_crossentropy'?\n",
        "We have sparse labels (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) with nothing in between, and these labels are mutually exclusive.\n",
        "\n",
        "If we had used one-hot encoding, such as [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3, we would need to use the \"categorical_crossentropy\" loss instead.\n",
        "\n",
        "If we were doing binary classification (e.g. patient vs control) with one or more binary labels (e.g. patient vs control, male vs female),  we would use the sigmoid (i.e. logistic) activation function in the output layer instead of the \"softmax\" function, and we would use the \"binary_crossentropy\" loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouMznHWCmwGe"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik1n3vz_mwGe"
      },
      "source": [
        "This is equivalent to:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCnQnKOgmwGf"
      },
      "source": [
        "```python\n",
        "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.SGD(),\n",
        "              metrics=[keras.metrics.sparse_categorical_accuracy])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2NATp2cmwGf"
      },
      "source": [
        "We have set up the data and model. Now we need to tell the learning algorithm how many epochs (i.e. one loop through all training samples) to run and set the batch size.\n",
        "Stochastic Gradient Descent performs a weight update for every batch (i.e. a subset) of training data, implying there are multiple weight updates per epoch instead of going through all samples and computing one weight update per epoch. This approach leads to a faster, more stable convergence. There is a discussion about optimal batch sizes: one strategy is to make batches as large as possible such that they fit in the memory of a GPU. This strategy optimizes parallelization. But there are claims that large batch sizes lead to instable learning (which could be countered by learning-rate warmup, i.e. start with smaller learning rates, increase the learning rate for a while and later decrease it again). The alternative, and that is the approach we take here is to use a batch size of 32 samples or less.\n",
        "\n",
        "Now we are ready to fit the model (and grab a coffee)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqZb0mzwmwGf"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32,\n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ESB_UsmwGf"
      },
      "source": [
        "Fitting a model provides us with a history object that contains a lot of information about the learning history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8apiG1efmwGf"
      },
      "outputs": [],
      "source": [
        "history.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHrcfYBkmwGf"
      },
      "outputs": [],
      "source": [
        "print(history.epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38pK1yGhmwGf"
      },
      "outputs": [],
      "source": [
        "history.history.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqwhr7i1mwGf"
      },
      "source": [
        "Most importantly, history contains a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set.\n",
        "You can create a pandas data frame from history.history and plot the learning curves below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcO8F5CYmwGf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(False)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "save_fig(\"keras_learning_curves_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgWKlrKjmwGf"
      },
      "source": [
        "Both the training accuracy and the validation accuracy steadily increase during training, while the training loss and the validation loss decrease. Moreover, since the validation curves are close to the training curves, there seems to be no problem with overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1E_y08HmwGg"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfR1UhNbmwGg"
      },
      "source": [
        "At this point our model predicts the labels of the validation dataset with an accuracy of 87.2%. Not too bad, but is this the best we can do?\n",
        "There are a couple of parameters that we can change before we even begin with the learning process. These are called hyperparameters, such as:\n",
        "- number of hidden layers\n",
        "- number of units per hidden layer\n",
        "- learning rate\n",
        "- leaning algorithm\n",
        "\n",
        "Although there are reasonable (and also unreasonable) assumptions you can start with, there is no apriory certainty about which combination of parameters will work best (as in \"produces the best generalization\"). Therefore, developing a useful machine learning model includes searching for the best set of hyperparameters. This is called hyperparameter tuning.\n",
        "\n",
        "To this aim we will start a new tensorflow session and write a function that creates a new model based on a given set of hyperparameters. Then we will search through the parameter space, which can be very large, in a loop that tests a given model and keeps in memory which model has fared best so far. This procedure should return this best model (already trained)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNc_6hKqmwGg"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHWTVRyYmwGg"
      },
      "outputs": [],
      "source": [
        "def build_model(n_hidden=2, n_neurons=30, learning_rate=3e-3, input_shape=[28, 28]):\n",
        "    print(n_hidden, n_neurons, learning_rate)\n",
        "    model = keras.models.Sequential()   \n",
        "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    #model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22YNjUQqmwGg"
      },
      "source": [
        "Scikit-learn has some smart procedures for hyperparameter tuning. Here we create a keras object that is accessible for scikit learn. [Unfortunately this function may be faulty leading to the cloning error at the end. Let's nevertheless take a look at how this can be implemented and become usable once the keras/scikit learn community can fix the underlying problem]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIWfLuEAmwGg"
      },
      "outputs": [],
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ywy4ODcmwGg"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#param_distribs = {\n",
        "#    #\"n_hidden\": [1, 2], #[0, 1, 2, 3],\n",
        "#    #\"n_neurons\": np.arange(1, 100),\n",
        "#    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "#}\n",
        "\n",
        "#create a dictionary of hyperparameters within certain ranges \n",
        "param_distribs = {\n",
        "    \"n_hidden\": [1, 2], #[0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(50, 300),\n",
        "    \"learning_rate\": [.1, .01, .001, .0001 , .00001],\n",
        "}\n",
        "\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
        "#you can speed this up by using all available cores on your computer (n_jobs=-1)\n",
        "#rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=30,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsCULIvWmwGh"
      },
      "outputs": [],
      "source": [
        "rnd_search_cv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxjLD51KmwGh"
      },
      "outputs": [],
      "source": [
        "rnd_search_cv.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H23BrImmwGh"
      },
      "outputs": [],
      "source": [
        "rnd_search_cv.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJHZ9ArfmwGh"
      },
      "outputs": [],
      "source": [
        "model = rnd_search_cv.best_estimator_.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBWNLrHlmwGh"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvU4nRXnmwGh"
      },
      "source": [
        "Let's assume we are satisfied with the validation accuracy of the original model (without hyperparameter tuning). The next step is to evaluate the model on the test data to see whether it generalizes well to data it has not seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2NV9zRHmwGh"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFv4qDnrmwGh"
      },
      "source": [
        "## Using the model to make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNF0SM26mwGh"
      },
      "source": [
        "Let's predict the labels of the first three elements from the test set. For each test item you will see ten probabilities, i.e. the probability that a given item belongs to each of the respective output classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07y9Nd5lmwGi"
      },
      "outputs": [],
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BupFiDFCmwGi"
      },
      "source": [
        "We can directly output the label of the winning class per test element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2q2C4sKmwGi"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict_classes(X_new)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjn7bV1pmwGi"
      },
      "source": [
        "We can also output the respective names of the winning classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU-wBWctmwGi"
      },
      "outputs": [],
      "source": [
        "np.array(class_names)[y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvRNf47-mwGi"
      },
      "outputs": [],
      "source": [
        "y_new = y_test[:3]\n",
        "y_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEJN6BuLmwGi"
      },
      "source": [
        "Last, but not least, we can plot the pictures for which we wanted to predict the labels and compare human and artificial intelligence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Jz4k6aVmwGi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7.2, 2.4))\n",
        "for index, image in enumerate(X_new):\n",
        "    plt.subplot(1, 3, index + 1)\n",
        "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y_test[index]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "save_fig('fashion_mnist_images_plot', tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av132vpsmwGi"
      },
      "source": [
        "Let's take a look at all the predictions of the independent test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCWwq1l3mwGi"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict_classes(X_test)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQN_1ZZCmwGi"
      },
      "source": [
        "It is always interesting to see, whether ther are systematic prediction errors. This can be visualized with a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0mZHRLumwGi"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4idiG5cmwGi"
      },
      "source": [
        "Imagine we want to see how well our model can classify handwritings of the digit \"5\". A two by two confusion matrix of (predicted vs actual class) allows for four options: true positives (\"5\" | 5), true negatives (\"not 5\" | not 5), false positives (\"5\" | not 5) and false negatives (\"not 5\" | 5).\n",
        "We can apply the same logic to the classification of fashion items and the confusion matrix above. \n",
        "\n",
        "From that we can compute different kinds of performance metrics:\n",
        "- Precision (accuracy of the positive predictions):\n",
        "precision = TP/(TP+FP)\n",
        "- Recall (aka sensitivity or true positive rate: recall = TP/(TP+FN)\n",
        "- F1 (harmonic meanof precision and recall, giving high weights to lower values. Thus, F1 will only be high, if both, precision and recall are high): F1 = TP/ (TP + (FN+FP)/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iXbxkWFmwGj"
      },
      "source": [
        "<img src=\"https://github.com/jvschw/ml4ni/blob/master/ML4NI/images/ann/confusion_mat.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ToSmKnkmwGj"
      },
      "source": [
        "sckit learn has a function for that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8yqbVRjmwGj"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "acc = statistics.mean((y_test==y_pred)*1.0)\n",
        "ps = precision_score(y_test, y_pred, average='weighted')\n",
        "rs = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print('accuracy: ', acc)\n",
        "print('precision:', ps)\n",
        "print('recall:   ', rs)\n",
        "print('F1:       ', f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW-Yw36QmwGj"
      },
      "source": [
        "Why does this yield accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mG_ts2JmwGj"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "acc = statistics.mean((y_test==y_pred)*1.0)\n",
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVKLV1BmwGj"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GDEo4VVmwGj"
      },
      "source": [
        "How well would a linear support vector classifier perform?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-x_m_OVmwGj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "1_introduction_to_neural_nets_with_keras.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}